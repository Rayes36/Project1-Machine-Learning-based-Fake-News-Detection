{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load csv file into a pandas dataframe\n",
        "df = pd.read_csv('/content/drive/MyDrive/WELFake_Dataset.csv', encoding='latin1')\n",
        "\n",
        "# Strip leading and trailing whitespace from the 'text' column\n",
        "df['text'] = df['text'].str.strip()\n",
        "df = df.dropna(subset=['number', 'title', 'text', 'label'], how='any')\n",
        "\n",
        "# Drop rows that do not have exactly 4 columns\n",
        "df = df.drop(df[df.apply(lambda row: len(row.dropna()) != 4, axis=1)].index)\n",
        "\n",
        "# Convert the `label` column to a numeric type\n",
        "df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
        "\n",
        "# Remove rows with non-numeric values in the `label` column\n",
        "df = df[pd.notnull(df['label'])]\n",
        "\n",
        "# Remove any columns named 'Unnamed'\n",
        "df = df.drop(columns=[col for col in df.columns if 'Unnamed' in col])\n",
        "\n",
        "# Reset index of the cleaned dataframe to start from 0\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Re-index the \"number\" column to start from 1 and increment by 1\n",
        "df['number'] = df.index + 1\n",
        "\n",
        "# Save the cleaned dataframe as a new csv file\n",
        "df.to_csv('/content/cleaned_WELFake_Dataset.csv', index=False)\n",
        "\n",
        "# Download the cleaned CSV file to your local machine\n",
        "from google.colab import files\n",
        "files.download('cleaned_WELFake_Dataset.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ac1KlpxLKua1",
        "outputId": "35a141f1-c8ee-4c54-efa5-92a80bc1de76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-171fc194c7cc>:4: DtypeWarning: Columns (0,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,544,545,546,547,548,549,550,551,552,553,554,555,557,558,559,560,562,563,567,568,569,570,571,572,573,575,576,577,578,579,580,582,583,584,586,587,588,589,590,591,593,594,595,596,597,598,599,600,602,603,604,605,606,608,609,610,611,613,614,615,616,618,619,620,622,623,624,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,676,677,678,679,680,681,682,684) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/drive/MyDrive/WELFake_Dataset.csv', encoding='latin1')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1568a6c3-6d32-4259-93c2-2b788ef77513\", \"cleaned_WELFake_Dataset.csv\", 242443275)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMo6UMv4b71l",
        "outputId": "35d92442-83b6-4b8e-b07f-469ee3c9b529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from langid) (1.22.4)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941187 sha256=8a4a7ec724fed8b9f99d4dbcd26548161d41e8b0baafcb2d3dcb31f3e33b7e10\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/95/a9/c292c9dd8cadb8f2359f1670ff198a40d47167b0be3236e1c8\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from langid.langid import LanguageIdentifier, model\n",
        "\n",
        "# Load the cleaned csv file into a pandas dataframe\n",
        "df = pd.read_csv('/content/cleaned_WELFake_Dataset.csv')\n",
        "\n",
        "# Remove any rows where the 'text' column is empty or not in English\n",
        "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
        "df = df[df['text'].apply(lambda x: x.strip() != '' and identifier.classify(x)[0] == 'en')]\n",
        "\n",
        "# Reset index of the filtered dataframe to start from 0\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Save the filtered dataframe as a new csv file\n",
        "df.to_csv('/content/filtered_WELFake_Dataset.csv', index=False)\n",
        "\n",
        "# Download the filtered CSV file to your local machine\n",
        "files.download('filtered_WELFake_Dataset.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_amkAVY6Y2qz",
        "outputId": "6bbe7d3b-c050-40d9-ba69-fca3d6f9eb14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f30f05dd-24dd-4b20-b909-af1a65bbe0da\", \"filtered_WELFake_Dataset.csv\", 239275912)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Loading the dataset\n",
        "df = pd.read_csv('/content/filtered_WELFake_Dataset.csv')\n",
        "\n",
        "# Preprocessing the text\n",
        "def preprocess_text(text):\n",
        "    # Tokenizing the text\n",
        "    words = word_tokenize(text.lower())\n",
        "    # Removing stop words\n",
        "    words = [word for word in words if not word in stop_words]\n",
        "    # Joining the remaining words\n",
        "    return \" \".join(words)\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Tagging the text\n",
        "tagged_data = [TaggedDocument(words=gensim.utils.simple_preprocess(title + \" \" + doc), tags=[i]) for i, (title, doc) in enumerate(zip(df['title'], df['processed_text']))]\n",
        "\n",
        "# Training the Doc2Vec model\n",
        "d2v_model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1, epochs=50)\n",
        "\n",
        "# Getting document vectors\n",
        "X = [d2v_model.docvecs[i] for i in range(len(df))]\n",
        "\n",
        "# Splitting the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Training and testing the model using logistic regression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Evaluating the performance of the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_lr))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_lr))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred_lr))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3giHtX_npaj",
        "outputId": "bb1fdf1f-e001-44e2-9b2d-9f60ca19bc22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-5-5cc74fb5e3ce>:37: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
            "  X = [d2v_model.docvecs[i] for i in range(len(df))]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9028233281049479\n",
            "Precision: 0.9070733104238259\n",
            "Recall: 0.8987090367428004\n",
            "F1-score: 0.9028718021805744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Loading the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/filtered_WELFake_Dataset.csv')\n",
        "\n",
        "# Preprocessing the text\n",
        "def preprocess_text(text):\n",
        "    # Tokenizing the text\n",
        "    words = word_tokenize(text.lower())\n",
        "    # Removing stop words\n",
        "    words = [word for word in words if not word in stop_words]\n",
        "    # Joining the remaining words\n",
        "    return \" \".join(words)\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "# Tagging the text\n",
        "tagged_data = [TaggedDocument(words=gensim.utils.simple_preprocess(title + \" \" + doc), tags=[i]) for i, (title, doc) in enumerate(zip(df['title'], df['processed_text']))]\n",
        "\n",
        "# Training the Doc2Vec model\n",
        "d2v_model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1, epochs=50)\n",
        "\n",
        "# Getting document vectors\n",
        "X = [d2v_model.docvecs[i] for i in range(len(df))]\n",
        "\n",
        "# Splitting the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Training and testing the model using XGBoost\n",
        "xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=100)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluating the performance of the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_xgb))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_xgb))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jls6BQePnzHh",
        "outputId": "2f02199f-e53d-49a8-e882-be1588e10764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "<ipython-input-1-57780fff5c32>:37: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
            "  X = [d2v_model.docvecs[i] for i in range(len(df))]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.898117781263368\n",
            "Precision: 0.8927872518870562\n",
            "Recall: 0.9060859696410838\n",
            "F1-score: 0.899387453354925\n"
          ]
        }
      ]
    }
  ]
}